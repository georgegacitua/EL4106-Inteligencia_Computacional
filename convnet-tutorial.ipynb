{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convnet-tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/georgegacitua/EL4106-Inteligencia_Computacional/blob/master/convnet-tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "9hI8tV5uVHZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Subir código cifar10.py. Basta con ejecutar una vez (inmune a reinicios de kernel de python, no de la máquina)"
      ]
    },
    {
      "metadata": {
        "id": "x7EKoANMPBps",
        "colab_type": "code",
        "outputId": "0dd56361-c7ec-4c00-d81e-5723ba9796a7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b89521b-eae2-4826-9da1-98762b6531e3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2b89521b-eae2-4826-9da1-98762b6531e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving cifar10.py to cifar10.py\n",
            "User uploaded file \"cifar10.py\" with length 6585 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pgIK9vPiSHU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Aquí comienza el cuerpo del código\n",
        "## Reinicie luego de entrenar cada modelo. Recuerde actualizar el directorio de tensorboard."
      ]
    },
    {
      "metadata": {
        "id": "vFrRwKUHOhs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "import tensorflow as tf\n",
        "\n",
        "from cifar10 import CIFAR10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVjsrPpMOhtI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.set_random_seed(1)\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYzZFJpM08iZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Constructor del dataset. Modifique aquí el *flag* de *data augmentation*"
      ]
    },
    {
      "metadata": {
        "id": "1Hk74ROkOhtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c361ce82-4a35-4522-d5b1-a4deef9f14c9"
      },
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "batch_size = 64\n",
        "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading CIFAR 10...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-b91thwOhtU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model blocks\n",
        "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
        "    # input_tensor b01c\n",
        "    # kernel_shape 01-in-out\n",
        "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
        "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
        "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
        "                             initializer=tf.constant_initializer(0.05))\n",
        "    \n",
        "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
        "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
        "    \n",
        "    # Other options are to use He et. al init. for weights and 0.01 \n",
        "    # to init. biases.\n",
        "    conv = tf.nn.conv2d(input_tensor, weights, \n",
        "                       strides = [1, 1, 1, 1], padding='SAME')\n",
        "    return tf.nn.relu(conv + biases)\n",
        "\n",
        "def fc_layer(input_tensor, weights_shape, layer_name):\n",
        "    # weights_shape in-out\n",
        "    weights = tf.get_variable(\"weights\", weights_shape,\n",
        "                              initializer = tf.contrib.layers.xavier_initializer())\n",
        "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
        "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
        "    mult_out = tf.matmul(input_tensor, weights)\n",
        "    return tf.nn.relu(mult_out+biases)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HCAGDv5M0hFU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Elija aquí el directorio donde guardará los registros de Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "yqn2fJpRUYkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PARENT_DIR = './summaries/'\n",
        "SUMMARIES_DIR = PARENT_DIR + 'conv_2_layer_with_dropout'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AaGDU7CF0p71",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Construcción del grafo"
      ]
    },
    {
      "metadata": {
        "id": "pOgKrUip2bwP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Construcción del modelo + función de costos"
      ]
    },
    {
      "metadata": {
        "id": "_Y0ezm__OhtY",
        "colab_type": "code",
        "outputId": "9599bcda-506d-44c1-d109-6ff609f5c1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# Model\n",
        "use_convnet = True\n",
        "n_conv_layers = 2\n",
        "\n",
        "n_filters_convs = [16, 32, 64]\n",
        "\n",
        "model_input = tf.placeholder(tf.float32, name='model_input', \n",
        "                             shape=(batch_size, 32, 32, 3))\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
        "\n",
        "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 10))\n",
        "\n",
        "if use_convnet:\n",
        "    layer_input = model_input\n",
        "    previous_n_feature_maps = 3\n",
        "    for layer_index in range(n_conv_layers):\n",
        "      layer_name = 'conv%d' % layer_index\n",
        "      with tf.variable_scope(layer_name):\n",
        "        conv_out = conv_layer(\n",
        "            layer_input, \n",
        "            [5, 5, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
        "            layer_name)\n",
        "      if layer_index == 0:\n",
        "        with tf.variable_scope(layer_name, reuse=True):\n",
        "          conv1_filters = tf.get_variable(\"weights\")\n",
        "          tf.summary.image(\n",
        "              'conv1_filters',\n",
        "              tf.transpose(conv1_filters, perm=[3, 0, 1, 2]),\n",
        "              max_outputs=n_filters_convs[layer_index]\n",
        "          )\n",
        "      previous_n_feature_maps = n_filters_convs[layer_index]\n",
        "      pool_out = tf.nn.max_pool(\n",
        "          conv_out, \n",
        "          ksize=[1, 2, 2, 1],\n",
        "          strides=[1, 2, 2, 1],\n",
        "          padding='SAME',\n",
        "          name='pool%d' % layer_index)\n",
        "      layer_input = pool_out\n",
        "     \n",
        "\n",
        "    fc_input = tf.layers.flatten(pool_out, name='fc_input')\n",
        "\n",
        "    feature_map_height = int(32 / (2**n_conv_layers))\n",
        "    \n",
        "    # First fully connected layer\n",
        "    layer_name = 'fc1'\n",
        "    with tf.variable_scope(layer_name):\n",
        "        fc1_out = fc_layer(\n",
        "            fc_input, \n",
        "            [(feature_map_height**2)*previous_n_feature_maps, 50], \n",
        "            layer_name)\n",
        "\n",
        "    fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
        "\n",
        "    # Second fully connected layer\n",
        "    layer_name = 'fc2'\n",
        "    with tf.variable_scope(layer_name):\n",
        "        fc2_out = fc_layer(fc1_out_drop, [50, 10], layer_name)\n",
        "    model_output = fc2_out\n",
        "        \n",
        "else:\n",
        "    # Reshape tensor to MLP\n",
        "    first_layer_input = tf.reshape(model_input, [-1,3072], name='first_layer_input')\n",
        "\n",
        "    # First layer\n",
        "    layer_name = 'fc1'\n",
        "    with tf.variable_scope(layer_name):\n",
        "        fc1_out = fc_layer(first_layer_input, [3072, 100], layer_name)\n",
        "\n",
        "    fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
        "\n",
        "    # Second layer\n",
        "    layer_name = 'fc2'\n",
        "    with tf.variable_scope(layer_name):\n",
        "        fc2_out = fc_layer(fc1_out_drop, [100, 10], layer_name)\n",
        "    model_output = fc2_out\n",
        "\n",
        "with tf.name_scope('loss_function'):\n",
        "    cross_entropy = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
        "                                           name='cross_entropy'))\n",
        "    xentropy_summary = tf.summary.scalar('cross_entropy', cross_entropy)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-c32800b08297>:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DAIrQw7r2E47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Construcción del optimizador + funciones auxiliares"
      ]
    },
    {
      "metadata": {
        "id": "wnMUAQYlOhtc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Optimization\n",
        "with tf.name_scope('optimizer'):\n",
        "    optimizer = tf.train.RMSPropOptimizer(0.0005)\n",
        "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
        "    optimizer.apply_gradients(grads_vars)\n",
        "    train_step = optimizer.minimize(cross_entropy)\n",
        "\n",
        "# Metrics\n",
        "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
        "                             tf.argmax(target, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
        "learning_summaries = tf.summary.merge((xentropy_summary, accuracy_summary))\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "# Useful training functions\n",
        "def validate():\n",
        "    cifar10.shuffleValidation()\n",
        "    batches = cifar10.getValidationSet(asBatches=True)\n",
        "    accs = []\n",
        "    xent_vals = []\n",
        "    for batch in batches:\n",
        "        data, labels = batch\n",
        "        acc, xentropy_val = sess.run((accuracy, cross_entropy),\n",
        "                       feed_dict={\n",
        "                model_input: data,\n",
        "                target: labels,\n",
        "                keep_prob: 1.0\n",
        "            })\n",
        "        accs.append(acc)\n",
        "        xent_vals.append(xentropy_val)\n",
        "    mean_xent = np.array(xent_vals).mean()    \n",
        "    mean_acc = np.array(accs).mean()\n",
        "    summary = sess.run(\n",
        "        merged,\n",
        "        feed_dict={\n",
        "            model_input: data,\n",
        "            target: labels,\n",
        "            keep_prob: 1.0\n",
        "        })\n",
        "    return summary, mean_acc, mean_xent\n",
        "def test():\n",
        "    batches = cifar10.getTestSet(asBatches=True)\n",
        "    accs = []\n",
        "    for batch in batches:\n",
        "        data, labels = batch\n",
        "        acc = sess.run(accuracy,\n",
        "                       feed_dict={\n",
        "                model_input: data,\n",
        "                target: labels,\n",
        "                keep_prob: 1.0\n",
        "            })\n",
        "        accs.append(acc)\n",
        "    mean_acc = np.array(accs).mean()\n",
        "    return mean_acc\n",
        "\n",
        "# Tensorboard writers\n",
        "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
        "                                     sess.graph)\n",
        "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUc5-KOHSALR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenar\n",
        "### Modifique el valor de *keep_prob* usado al computar *train_step* para activar/desactivar el *dropout*.\n",
        "### NO MODIFIQUE EL KEEP_PROB USADO EN OTROS SITIOS, ESOS CORRESPONDEN A INFERENCIA."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "TRKMxMztOhtg",
        "colab_type": "code",
        "outputId": "a046594e-0acf-4b94-dcde-436e0dc61a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        }
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "cifar10.reset()\n",
        "print(\"Trainable variables\")\n",
        "for n in tf.trainable_variables():\n",
        "    print(n.name)\n",
        "if use_convnet:\n",
        "    epochs = 30\n",
        "else:\n",
        "    epochs = 50\n",
        "    \n",
        "t_i = time.time()\n",
        "n_batches = cifar10.n_batches\n",
        "val_acc_vals = []\n",
        "test_acc_vals = []\n",
        "while cifar10.getEpoch() < epochs:\n",
        "    epoch = cifar10.getEpoch()\n",
        "    batch, batch_idx = cifar10.nextBatch()\n",
        "    batch_data = batch[0]\n",
        "    batch_labels = batch[1]\n",
        "    \n",
        "    # just a training iteration\n",
        "    _ = sess.run(train_step,\n",
        "                feed_dict={\n",
        "            model_input: batch_data,\n",
        "            target: batch_labels,\n",
        "            keep_prob: 0.5 ###### Modifique el dropout aqui y solo aqui. #####\n",
        "        })\n",
        "    \n",
        "    step = batch_idx+epoch*n_batches\n",
        "    \n",
        "    # Write training summary\n",
        "    if step%50==0:\n",
        "        summary = sess.run(learning_summaries,\n",
        "                          feed_dict={\n",
        "                model_input: batch_data,\n",
        "                target: batch_labels,\n",
        "                keep_prob: 1.0 # set to 1.0 at inference time\n",
        "            })\n",
        "        train_writer.add_summary(summary, step)\n",
        "        \n",
        "    # gradient (by layer) statistics over last training batch & validation summary\n",
        "    if batch_idx==0:\n",
        "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
        "                      feed_dict={\n",
        "            model_input: batch_data,\n",
        "            target: batch_labels,\n",
        "            keep_prob: 1.0\n",
        "        })\n",
        "        \n",
        "        summary, validation_accuracy, validation_loss = validate()\n",
        "        validation_writer.add_summary(summary, step)\n",
        "        print('[Epoch %d, it %d] Training acc. %.3f, loss %.3f. \\\n",
        "Valid. acc. %.3f, loss %.3f' % (\n",
        "            epoch,\n",
        "            step,\n",
        "            acc,\n",
        "            loss,\n",
        "            validation_accuracy,\n",
        "            validation_loss\n",
        "        ))\n",
        "        val_acc_vals.append(validation_accuracy)\n",
        "        test_accuracy = test()\n",
        "        test_acc_vals.append(test_accuracy)\n",
        "        print(\"Time elapsed %.2f minutes\" % ((time.time()-t_i)/60.0))\n",
        "train_writer.flush()\n",
        "validation_writer.flush()\n",
        "\n",
        "val_acc_vals = np.array(val_acc_vals)\n",
        "test_acc_vals = np.array(test_acc_vals)\n",
        "best_epoch = np.argmax(val_acc_vals)\n",
        "test_acc_at_best = test_acc_vals[best_epoch]\n",
        "print('*'*30)\n",
        "print(\"Testing set accuracy @ epoch %d (best validation acc): %.4f\" % (best_epoch, test_acc_at_best))\n",
        "print('*'*30)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable variables\n",
            "conv0/weights:0\n",
            "conv0/biases:0\n",
            "conv1/weights:0\n",
            "conv1/biases:0\n",
            "fc1/weights:0\n",
            "fc1/biases:0\n",
            "fc2/weights:0\n",
            "fc2/biases:0\n",
            "[Epoch 0, it 0] Training acc. 0.141, loss 2.329. Valid. acc. 0.090, loss 2.345\n",
            "Time elapsed 0.03 minutes\n",
            "[Epoch 1, it 703] Training acc. 0.453, loss 1.560. Valid. acc. 0.467, loss 1.525\n",
            "Time elapsed 0.12 minutes\n",
            "[Epoch 2, it 1406] Training acc. 0.719, loss 0.998. Valid. acc. 0.565, loss 1.263\n",
            "Time elapsed 0.21 minutes\n",
            "[Epoch 3, it 2109] Training acc. 0.719, loss 0.943. Valid. acc. 0.582, loss 1.200\n",
            "Time elapsed 0.29 minutes\n",
            "[Epoch 4, it 2812] Training acc. 0.656, loss 1.017. Valid. acc. 0.622, loss 1.119\n",
            "Time elapsed 0.38 minutes\n",
            "[Epoch 5, it 3515] Training acc. 0.750, loss 0.748. Valid. acc. 0.640, loss 1.056\n",
            "Time elapsed 0.46 minutes\n",
            "[Epoch 6, it 4218] Training acc. 0.797, loss 0.874. Valid. acc. 0.651, loss 1.004\n",
            "Time elapsed 0.55 minutes\n",
            "[Epoch 7, it 4921] Training acc. 0.734, loss 0.706. Valid. acc. 0.667, loss 0.975\n",
            "Time elapsed 0.63 minutes\n",
            "[Epoch 8, it 5624] Training acc. 0.719, loss 0.769. Valid. acc. 0.655, loss 1.002\n",
            "Time elapsed 0.72 minutes\n",
            "[Epoch 9, it 6327] Training acc. 0.734, loss 0.788. Valid. acc. 0.669, loss 0.962\n",
            "Time elapsed 0.81 minutes\n",
            "[Epoch 10, it 7030] Training acc. 0.734, loss 0.703. Valid. acc. 0.675, loss 0.963\n",
            "Time elapsed 0.89 minutes\n",
            "[Epoch 11, it 7733] Training acc. 0.766, loss 0.605. Valid. acc. 0.680, loss 0.942\n",
            "Time elapsed 0.98 minutes\n",
            "[Epoch 12, it 8436] Training acc. 0.719, loss 0.877. Valid. acc. 0.679, loss 0.951\n",
            "Time elapsed 1.07 minutes\n",
            "[Epoch 13, it 9139] Training acc. 0.844, loss 0.576. Valid. acc. 0.688, loss 0.940\n",
            "Time elapsed 1.15 minutes\n",
            "[Epoch 14, it 9842] Training acc. 0.875, loss 0.459. Valid. acc. 0.682, loss 0.938\n",
            "Time elapsed 1.24 minutes\n",
            "[Epoch 15, it 10545] Training acc. 0.922, loss 0.491. Valid. acc. 0.689, loss 0.925\n",
            "Time elapsed 1.33 minutes\n",
            "[Epoch 16, it 11248] Training acc. 0.812, loss 0.680. Valid. acc. 0.690, loss 0.934\n",
            "Time elapsed 1.41 minutes\n",
            "[Epoch 17, it 11951] Training acc. 0.750, loss 0.861. Valid. acc. 0.682, loss 0.945\n",
            "Time elapsed 1.50 minutes\n",
            "[Epoch 18, it 12654] Training acc. 0.891, loss 0.420. Valid. acc. 0.687, loss 0.965\n",
            "Time elapsed 1.58 minutes\n",
            "[Epoch 19, it 13357] Training acc. 0.828, loss 0.538. Valid. acc. 0.687, loss 0.972\n",
            "Time elapsed 1.67 minutes\n",
            "[Epoch 20, it 14060] Training acc. 0.891, loss 0.310. Valid. acc. 0.700, loss 0.956\n",
            "Time elapsed 1.76 minutes\n",
            "[Epoch 21, it 14763] Training acc. 0.844, loss 0.561. Valid. acc. 0.690, loss 0.953\n",
            "Time elapsed 1.84 minutes\n",
            "[Epoch 22, it 15466] Training acc. 0.844, loss 0.546. Valid. acc. 0.697, loss 0.934\n",
            "Time elapsed 1.93 minutes\n",
            "[Epoch 23, it 16169] Training acc. 0.797, loss 0.613. Valid. acc. 0.698, loss 0.957\n",
            "Time elapsed 2.01 minutes\n",
            "[Epoch 24, it 16872] Training acc. 0.766, loss 0.709. Valid. acc. 0.695, loss 0.953\n",
            "Time elapsed 2.10 minutes\n",
            "[Epoch 25, it 17575] Training acc. 0.891, loss 0.465. Valid. acc. 0.691, loss 0.969\n",
            "Time elapsed 2.18 minutes\n",
            "[Epoch 26, it 18278] Training acc. 0.812, loss 0.485. Valid. acc. 0.694, loss 0.969\n",
            "Time elapsed 2.27 minutes\n",
            "[Epoch 27, it 18981] Training acc. 0.844, loss 0.545. Valid. acc. 0.691, loss 0.993\n",
            "Time elapsed 2.35 minutes\n",
            "[Epoch 28, it 19684] Training acc. 0.750, loss 0.745. Valid. acc. 0.684, loss 0.966\n",
            "Time elapsed 2.44 minutes\n",
            "[Epoch 29, it 20387] Training acc. 0.781, loss 0.535. Valid. acc. 0.677, loss 1.038\n",
            "Time elapsed 2.52 minutes\n",
            "******************************\n",
            "Testing set accuracy @ epoch 20 (best validation acc): 0.6985\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h554rbJcR7Jj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizar en Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "aL3_pq6eOhtl",
        "colab_type": "code",
        "outputId": "0e1b62cf-25c3-414f-9050-fbd3c0885b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2006
        }
      },
      "cell_type": "code",
      "source": [
        "# ----- Descarga de ngrok para crear tunel\n",
        "%%bash\n",
        "file=\"ngrok-stable-linux-amd64.zip\"\n",
        "if [ -f \"$file\" ]\n",
        "then\n",
        "\techo \"$file already downloaded.\"\n",
        "else\n",
        "    wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    unzip ngrok-stable-linux-amd64.zip\n",
        "fi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2018-10-19 01:49:49--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.2.175.150, 52.201.75.180, 52.0.94.50, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.2.175.150|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0%  794K 7s\n",
            "    50K .......... .......... .......... .......... ..........  1% 1.57M 5s\n",
            "   100K .......... .......... .......... .......... ..........  2% 31.8M 3s\n",
            "   150K .......... .......... .......... .......... ..........  3% 69.5M 2s\n",
            "   200K .......... .......... .......... .......... ..........  4% 1.64M 3s\n",
            "   250K .......... .......... .......... .......... ..........  5% 67.1M 2s\n",
            "   300K .......... .......... .......... .......... ..........  6% 59.9M 2s\n",
            "   350K .......... .......... .......... .......... ..........  7% 62.4M 2s\n",
            "   400K .......... .......... .......... .......... ..........  8% 1.72M 2s\n",
            "   450K .......... .......... .......... .......... ..........  9%  101M 1s\n",
            "   500K .......... .......... .......... .......... .......... 10% 58.8M 1s\n",
            "   550K .......... .......... .......... .......... .......... 11% 70.0M 1s\n",
            "   600K .......... .......... .......... .......... .......... 12% 81.9M 1s\n",
            "   650K .......... .......... .......... .......... .......... 13%  105M 1s\n",
            "   700K .......... .......... .......... .......... .......... 14%  107M 1s\n",
            "   750K .......... .......... .......... .......... .......... 15% 95.1M 1s\n",
            "   800K .......... .......... .......... .......... .......... 16% 99.8M 1s\n",
            "   850K .......... .......... .......... .......... .......... 17% 1.83M 1s\n",
            "   900K .......... .......... .......... .......... .......... 18% 78.9M 1s\n",
            "   950K .......... .......... .......... .......... .......... 19% 59.8M 1s\n",
            "  1000K .......... .......... .......... .......... .......... 20% 59.9M 1s\n",
            "  1050K .......... .......... .......... .......... .......... 21% 80.5M 1s\n",
            "  1100K .......... .......... .......... .......... .......... 21%  109M 1s\n",
            "  1150K .......... .......... .......... .......... .......... 22% 99.3M 1s\n",
            "  1200K .......... .......... .......... .......... .......... 23%  101M 1s\n",
            "  1250K .......... .......... .......... .......... .......... 24%  104M 1s\n",
            "  1300K .......... .......... .......... .......... .......... 25% 94.6M 1s\n",
            "  1350K .......... .......... .......... .......... .......... 26%  105M 1s\n",
            "  1400K .......... .......... .......... .......... .......... 27% 98.4M 1s\n",
            "  1450K .......... .......... .......... .......... .......... 28% 94.2M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 29%  109M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 30%  109M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 31%  100M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 32% 2.12M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 33% 59.1M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 34% 45.4M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 35% 66.1M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 36% 69.0M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 37% 79.2M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 38%  111M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 39% 96.2M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 40%  155M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 41%  113M 0s\n",
            "  2150K .......... .......... .......... .......... .......... 42%  143M 0s\n",
            "  2200K .......... .......... .......... .......... .......... 42%  170M 0s\n",
            "  2250K .......... .......... .......... .......... .......... 43%  136M 0s\n",
            "  2300K .......... .......... .......... .......... .......... 44%  160M 0s\n",
            "  2350K .......... .......... .......... .......... .......... 45%  119M 0s\n",
            "  2400K .......... .......... .......... .......... .......... 46%  154M 0s\n",
            "  2450K .......... .......... .......... .......... .......... 47%  178M 0s\n",
            "  2500K .......... .......... .......... .......... .......... 48%  146M 0s\n",
            "  2550K .......... .......... .......... .......... .......... 49%  147M 0s\n",
            "  2600K .......... .......... .......... .......... .......... 50%  158M 0s\n",
            "  2650K .......... .......... .......... .......... .......... 51%  163M 0s\n",
            "  2700K .......... .......... .......... .......... .......... 52%  107M 0s\n",
            "  2750K .......... .......... .......... .......... .......... 53%  102M 0s\n",
            "  2800K .......... .......... .......... .......... .......... 54% 92.9M 0s\n",
            "  2850K .......... .......... .......... .......... .......... 55% 97.0M 0s\n",
            "  2900K .......... .......... .......... .......... .......... 56% 94.9M 0s\n",
            "  2950K .......... .......... .......... .......... .......... 57% 94.7M 0s\n",
            "  3000K .......... .......... .......... .......... .......... 58% 90.9M 0s\n",
            "  3050K .......... .......... .......... .......... .......... 59%  102M 0s\n",
            "  3100K .......... .......... .......... .......... .......... 60% 85.3M 0s\n",
            "  3150K .......... .......... .......... .......... .......... 61% 2.72M 0s\n",
            "  3200K .......... .......... .......... .......... .......... 62% 85.9M 0s\n",
            "  3250K .......... .......... .......... .......... .......... 63% 48.8M 0s\n",
            "  3300K .......... .......... .......... .......... .......... 63% 92.2M 0s\n",
            "  3350K .......... .......... .......... .......... .......... 64% 70.7M 0s\n",
            "  3400K .......... .......... .......... .......... .......... 65% 65.4M 0s\n",
            "  3450K .......... .......... .......... .......... .......... 66% 98.5M 0s\n",
            "  3500K .......... .......... .......... .......... .......... 67% 91.3M 0s\n",
            "  3550K .......... .......... .......... .......... .......... 68%  104M 0s\n",
            "  3600K .......... .......... .......... .......... .......... 69% 85.8M 0s\n",
            "  3650K .......... .......... .......... .......... .......... 70%  105M 0s\n",
            "  3700K .......... .......... .......... .......... .......... 71% 92.5M 0s\n",
            "  3750K .......... .......... .......... .......... .......... 72%  104M 0s\n",
            "  3800K .......... .......... .......... .......... .......... 73% 98.5M 0s\n",
            "  3850K .......... .......... .......... .......... .......... 74%  101M 0s\n",
            "  3900K .......... .......... .......... .......... .......... 75% 96.9M 0s\n",
            "  3950K .......... .......... .......... .......... .......... 76% 97.2M 0s\n",
            "  4000K .......... .......... .......... .......... .......... 77%  108M 0s\n",
            "  4050K .......... .......... .......... .......... .......... 78%  110M 0s\n",
            "  4100K .......... .......... .......... .......... .......... 79%  110M 0s\n",
            "  4150K .......... .......... .......... .......... .......... 80% 96.4M 0s\n",
            "  4200K .......... .......... .......... .......... .......... 81% 94.3M 0s\n",
            "  4250K .......... .......... .......... .......... .......... 82%  113M 0s\n",
            "  4300K .......... .......... .......... .......... .......... 83% 91.8M 0s\n",
            "  4350K .......... .......... .......... .......... .......... 84%  103M 0s\n",
            "  4400K .......... .......... .......... .......... .......... 84%  109M 0s\n",
            "  4450K .......... .......... .......... .......... .......... 85% 96.3M 0s\n",
            "  4500K .......... .......... .......... .......... .......... 86% 94.5M 0s\n",
            "  4550K .......... .......... .......... .......... .......... 87% 97.8M 0s\n",
            "  4600K .......... .......... .......... .......... .......... 88% 98.9M 0s\n",
            "  4650K .......... .......... .......... .......... .......... 89%  107M 0s\n",
            "  4700K .......... .......... .......... .......... .......... 90% 95.5M 0s\n",
            "  4750K .......... .......... .......... .......... .......... 91%  100M 0s\n",
            "  4800K .......... .......... .......... .......... .......... 92% 92.6M 0s\n",
            "  4850K .......... .......... .......... .......... .......... 93%  103M 0s\n",
            "  4900K .......... .......... .......... .......... .......... 94%  109M 0s\n",
            "  4950K .......... .......... .......... .......... .......... 95%  114M 0s\n",
            "  5000K .......... .......... .......... .......... .......... 96% 96.4M 0s\n",
            "  5050K .......... .......... .......... .......... .......... 97%  110M 0s\n",
            "  5100K .......... .......... .......... .......... .......... 98% 87.8M 0s\n",
            "  5150K .......... .......... .......... .......... .......... 99% 93.6M 0s\n",
            "  5200K .......... .......... .......... .......              100% 97.8M=0.3s\n",
            "\n",
            "2018-10-19 01:49:50 (18.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "baPlJx8YT2IL",
        "colab_type": "code",
        "outputId": "cd7e5afc-f665-403b-e3a5-7ff3a6d8eade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# ----- Ejecutar despues de nueva corrida para actualizar Tensorboard\n",
        "print(\"Showing summaries at %s\" % (PARENT_DIR))\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(PARENT_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "print('Click URL to open TensorBoard:')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Showing summaries at ./summaries/\n",
            "Click URL to open TensorBoard:\n",
            "http://68a79835.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CUYkbGcHT3Vz",
        "colab_type": "code",
        "outputId": "914695b8-1744-4a24-eda8-122225ec5b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ls\n",
        "ps aux | grep -e 'tensorboard'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-py\n",
            "cifar10.py\n",
            "ngrok\n",
            "ngrok-stable-linux-amd64.zip\n",
            "__pycache__\n",
            "sample_data\n",
            "summaries\n",
            "root         150  5.1  2.1 2028280 284400 ?      Sl   01:49   0:03 /usr/bin/python2 /usr/local/bin/tensorboard --logdir ./summaries/ --host 0.0.0.0 --port 6006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N1KeC-1MBKNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}