{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmbRYKD1odtB"
   },
   "source": [
    "# Tarea 1: Perceptrón Multicapa\n",
    "### EL4106 Inteligencia Computacional\n",
    "\n",
    "Profesor de Cátedra: Pablo Estévez<br>\n",
    "Profesor Auxiliar: Ignacio Reyes<br>\n",
    "Ayudantes: Germán García, Esteban Reyes, Nicolás Tapia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr3dCq8pYd_w"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wiv1sT9tpZO-"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix as sk_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAqEQZkkRgTJ"
   },
   "source": [
    "## Preparación de la base de datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwD-4ePxRuzZ"
   },
   "outputs": [],
   "source": [
    "# ----- Digito a identificar.\n",
    "RUT_veri_number = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RahVklIFRfbE"
   },
   "outputs": [],
   "source": [
    "def process_dataset(images, labels, selected_class):\n",
    "    \"\"\"Revuelve datos y selecciona subconjunto segun la clase seleccionada.\n",
    "    \n",
    "    Los datos de imagenes y etiquetas son revueltos, se seleccionan aquellos\n",
    "    que coinciden con la etiqueta de la clase seleccionada, y un subconjunto\n",
    "    del mismo tamaño que el anterior es seleccionado de entre todas las demas\n",
    "    clases para obtener un problema balanceado.\n",
    "    \"\"\"\n",
    "    shuffled_indexes = np.random.permutation(len(labels))  # Primer shuffle\n",
    "    images = images[shuffled_indexes]\n",
    "    labels = labels[shuffled_indexes]\n",
    "    selected_column = labels[:, selected_class]\n",
    "    selected_images_indexes = np.where(selected_column == 1)[0]\n",
    "    selected_size = len(selected_images_indexes)\n",
    "    non_selected_indexes_subset = np.where(selected_column == 0)[0][:selected_size]\n",
    "    indexes = np.concatenate(\n",
    "        (selected_images_indexes, \n",
    "        non_selected_indexes_subset),\n",
    "        axis=0)\n",
    "    # No queremos que el modelo primero vea todos los datos de una clase y\n",
    "    # despues todos los de la otra, asi que volvemos a revolver.\n",
    "    np.random.shuffle(indexes)\n",
    "    images_subset = images[indexes]\n",
    "    labels_subset = selected_column[indexes]\n",
    "    labels_subset = np.array(labels_subset, dtype=np.int32)\n",
    "    return images_subset, labels_subset\n",
    "\n",
    "\n",
    "# ----- Carga de la base de datos MNIST\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)  \n",
    "\n",
    "# ----- Preprocesamiento de datos de entrenamiento, validacion, y test\n",
    "training_images, training_labels = process_dataset(\n",
    "    mnist.train.images,\n",
    "    mnist.train.labels,\n",
    "    RUT_veri_number)\n",
    "validation_images, validation_labels = process_dataset(\n",
    "    mnist.validation.images,\n",
    "    mnist.validation.labels,\n",
    "    RUT_veri_number)\n",
    "testing_images, testing_labels = process_dataset(\n",
    "    mnist.test.images,\n",
    "    mnist.test.labels,\n",
    "    RUT_veri_number)\n",
    "print('Processing ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLcbP5NJRyH_"
   },
   "outputs": [],
   "source": [
    "# ----- Visualizacion de algunas imagenes de los datos de entrenamiento\n",
    "\n",
    "chosen_idx = np.random.choice(training_images.shape[0], size=6, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(10,5))\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image = training_images[idx, :]\n",
    "    digit = training_labels[idx]\n",
    "    ax[i].imshow(image.reshape((28, 28)))\n",
    "    ax[i].set_title(\"Etiqueta: %d\" % digit)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4f4FFyhpdDA"
   },
   "source": [
    "## Definición de Clasificador MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nfe-XLlTWnsf"
   },
   "source": [
    "### Función del modelo de red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MS9viD6ipcIt"
   },
   "outputs": [],
   "source": [
    "def model_fn(inputs, layer_sizes):\n",
    "    \"\"\"Construye el grafo computacional de la red MLP.\n",
    "    \n",
    "    Se procesa 'inputs' a través de un perceptron multicapa, cuya salida se\n",
    "    retorna en forma de logits y probabilidades de cada clase.\n",
    "    Summaries son agregados para su visualizacion en Tensorboard.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Tensor de entrada de dimensiones (batch_size, n_features).\n",
    "        layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "            neuronas. La salida de la capa i-esima posee dimensiones\n",
    "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "    \n",
    "    Returns:\n",
    "        logits: Tensor de salida lineal de dimensiones (batch_size, n_classes).\n",
    "        probabilities: Tensor de salida con activacion softmax.\n",
    "    \"\"\"\n",
    "\n",
    "    layer = inputs\n",
    "    n_layers = len(layer_sizes)   \n",
    "    # Capas neuronales\n",
    "    for i in range(n_layers):\n",
    "        with tf.variable_scope('layer_'+str(i)):          \n",
    "            previous_size = layer.shape[1].value  # Tamaño de entrada a la capa          \n",
    "            # Pesos de la capa oculta i-esima\n",
    "            weights = tf.get_variable(\n",
    "                name='weights_'+str(i),\n",
    "                shape=[previous_size, layer_sizes[i]],\n",
    "                initializer=tf.glorot_uniform_initializer())\n",
    "            # Summary de la distribucion de los pesos\n",
    "            tf.summary.histogram('weights_'+str(i), weights)\n",
    "\n",
    "            # Sesgos de la capa oculta i-esima\n",
    "            biases = tf.get_variable(\n",
    "                name='biases_'+str(i),\n",
    "                shape=[layer_sizes[i]],\n",
    "                initializer=tf.zeros_initializer())\n",
    "            # Summary de la distribucion de los sesgos\n",
    "            tf.summary.histogram('biases_'+str(i), biases)\n",
    "            # Aplicacion de pesos y sesgos\n",
    "            layer = tf.matmul(layer, weights) + biases          \n",
    "            if i < n_layers - 1:\n",
    "                # Aplicacion de funcion de activacion de capa oculta\n",
    "                layer = tf.nn.sigmoid(layer)\n",
    "            else:\n",
    "                # Aplicacion de funcion de activacion de capa de salida\n",
    "                logits = layer\n",
    "                probabilities = tf.nn.softmax(logits)   \n",
    "    return logits, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Bip-9FV1_WJ"
   },
   "source": [
    "### Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjgsGFG31-no"
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels, loss_function_name):\n",
    "    \"\"\"Construye el grafo computacional del calculo de la funcion de costo.\n",
    "    \n",
    "    Se aplica el loss 'loss_function_name' entre los labels reales y la salida\n",
    "    de la MLP. Ademas, se calcula el accuracy.\n",
    "    Summaries son agregados para su visualizacion en Tensorboard.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor de dimensiones (batch_size, n_classes) con los logits\n",
    "        de la salida de la MLP\n",
    "        labels: Tensor de dimensiones (batch_size,) con las etiquetas reales.\n",
    "        loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Tensor escalar que corresponde al costo calculado.\n",
    "        accuracy: Tensor escalar que corresponde al accuracy calculado.\n",
    "        val_summaries: summaries que son de interes al predecir en la validacion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Codificacion 'one hot' para las etiquetas de clase\n",
    "    n_classes = logits.shape[1].value\n",
    "    one_hot_labels = tf.one_hot(labels, n_classes)\n",
    "    val_summaries = []\n",
    "    with tf.variable_scope('loss'):\n",
    "        if loss_function_name == 'cross_entropy':\n",
    "            # Cross Entropy loss\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                    logits=logits,\n",
    "                    labels=one_hot_labels),\n",
    "                name='xentropy'\n",
    "            )\n",
    "            # Summary de loss\n",
    "            loss_sum = tf.summary.scalar('xentropy_loss', loss)\n",
    "        elif loss_function_name == 'mse':\n",
    "            # Mean Squared Error loss\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.square(one_hot_labels - probabilities),\n",
    "                name='mse'\n",
    "            )\n",
    "            # Summary de loss\n",
    "            loss_sum = tf.summary.scalar('mse_loss', loss)\n",
    "        else:\n",
    "            raise ValueError('Wrong value for loss_function_name')\n",
    "    val_summaries.append(loss_sum)\n",
    "    with tf.variable_scope('accuracy'):\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        correct_predictions = tf.equal(labels, predictions)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "        # Summary de accuracy\n",
    "        acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    val_summaries.append(acc_sum)\n",
    "    return loss, accuracy, val_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXb1NDd96-4r"
   },
   "source": [
    "### Función del optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnngKAFgv7kZ"
   },
   "outputs": [],
   "source": [
    "def optimizer_fn(loss, learning_rate):\n",
    "    \"\"\"Construye el grafo computacional de la actualizacion por gradiente.\n",
    "    \n",
    "    Se aplica el algoritmo de optimizacion 'sgd' para ejecutar una\n",
    "    iteracion de minimizacion por gradiente sobre el loss entregado.\n",
    "    \n",
    "    Args:\n",
    "        loss: Tensor escalar que corresponde al costo calculado.\n",
    "        learning_rate: Escalar que indica la tasa de aprendizaje. Al seleccionar\n",
    "            Adam este parametro es ignorado.\n",
    "        optimizer_name: 'sgd' o 'adam', selecciona el optimizador.\n",
    "    \n",
    "    Returns:\n",
    "        train_step: Operacion que ejecuta una iteracion de gradiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope('optimizer'):\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # Minimizacion de la funcion de costo con el optimizador elegido\n",
    "        train_step = optimizer.minimize(loss)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx8DACFS8TxR"
   },
   "source": [
    "###Clase de Clasificador MLP\n",
    "Esta clase utiliza las funciones anteriores para la construcción del grafo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fITl9UaSydAf"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(object):\n",
    "    \"\"\"Implementacion de clasificador Perceptron Multicapa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        layer_sizes,\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=None,\n",
    "        logdir='logs'\n",
    "    ):\n",
    "        \"\"\"Construye un clasificador Perceptron Multicapa.\n",
    "        \n",
    "        Args:\n",
    "            n_features: Entero que indica el numero de caracteristicas de las entradas.\n",
    "            layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "            neuronas. La salida de la capa i-esima posee dimensiones\n",
    "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "            loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "                Por defecto es 'cross_entropy'.\n",
    "            learning_rate: Escalar que indica la tasa de aprendizaje. Al\n",
    "                seleccionar Adam este parametro es ignorado. Por defecto es 0.1\n",
    "            batch_size: Entero que indica el tamaño de los mini-batches para\n",
    "                el entrenamiento de la red.\n",
    "            max_epochs: Entero que indica el maximo numero de epocas de\n",
    "                entrenamiento (pasadas completas por los datos de entrada) \n",
    "            early_stopping: Indica cuantas veces las verificaciones en la\n",
    "                validacion deben indicar que el costo esta aumentando para\n",
    "                realizar una detencion temprana. Por defecto es None, lo cual\n",
    "                desactiva la detencion temprana.\n",
    "            logdir: String que indica el directorio en donde guardar los\n",
    "                archivos del entrenamiento. Por defecto es 'logs'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Limpiar grafo computacional\n",
    "        tf.reset_default_graph()\n",
    "        # Agregar parametros al objeto\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.logdir = logdir\n",
    "        # Tensor que reserva espacio para las imagenes de entrada a la red\n",
    "        self.inputs_ph = tf.placeholder(tf.float32, shape=[None, n_features],\n",
    "                                   name='image_placeholder')\n",
    "        # Tensor que reserva espacio para las etiquetas de la entrada\n",
    "        self.labels_ph = tf.placeholder(tf.int32, shape=None,\n",
    "                                        name='label_placeholder')\n",
    "        # Construccion del grafo computacional\n",
    "        self.logits, self.proba = model_fn(\n",
    "            self.inputs_ph, layer_sizes)\n",
    "        self.loss, self.accuracy, self.val_summ = loss_fn(\n",
    "            self.logits, self.labels_ph, loss_function_name)\n",
    "        self.train_step = optimizer_fn(\n",
    "            self.loss, learning_rate)\n",
    "        # Fusion de todos los summaries\n",
    "        self.summ = tf.summary.merge_all()\n",
    "        self.val_summ = tf.summary.merge(self.val_summ)\n",
    "        # Crear sesion de tensorflow para administrar grafo\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrenamiento del clasificador con los hiperparametros escogidos.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Entradas del entrenamiento con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas del entrenamiento con dimensiones (n_ejemplos,)\n",
    "            X_train: Entradas de la validacion con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas de la validacion con dimensiones (n_ejemplos,)\n",
    "            \n",
    "        Returns:\n",
    "            train_stats: Diccionario con datos historicos del entrenamiento.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Creacion de 'writers' que guardan datos para Tensorboard\n",
    "        writer_train = tf.summary.FileWriter(self.logdir + '/train')\n",
    "        writer_val = tf.summary.FileWriter(self.logdir + '/val')\n",
    "        writer_train.add_graph(self.sess.graph)\n",
    "        print('\\n\\n[Beginning training of MLP at logdir \"%s\"]\\n' % (self.logdir,))    \n",
    "        # Inicializacion de todas las variables\n",
    "        self.sess.run(tf.global_variables_initializer())     \n",
    "        # Definicion de variables utiles para el entrenamiento\n",
    "        n_batches = int(X_train.shape[0] / self.batch_size)\n",
    "        prev_validation_loss = 100.0\n",
    "        validation_period = 10\n",
    "        early_stop_flag = False\n",
    "        start_time = time.time()\n",
    "        iteration_history = []\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        # Ciclo que recorre una epoca completa de los datos cada vez\n",
    "        for epoch in range(self.max_epochs):\n",
    "            if early_stop_flag:\n",
    "                # Si early stopping se activo, detener el entrenamiento\n",
    "                break\n",
    "            # Para cada nueva epoca, hacer un shuffle al set de train\n",
    "            new_indexes = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[new_indexes, :]\n",
    "            y_train = y_train[new_indexes]\n",
    "            \n",
    "            # Ciclo que recorre los mini batches del set de train\n",
    "            for i in range(n_batches):\n",
    "                if early_stop_flag:\n",
    "                    # Si early stopping se activo, detener el entrenamiento\n",
    "                    break  \n",
    "                iteration = epoch * n_batches + i           \n",
    "                # Obtencion del minibatch actual\n",
    "                start = i * self.batch_size\n",
    "                end = (i+1) * self.batch_size\n",
    "                X_batch = X_train[start:end, :]\n",
    "                y_batch = y_train[start:end]          \n",
    "                # Ejecutar una iteracion de gradiente\n",
    "                feed_dict = {self.inputs_ph: X_batch, self.labels_ph: y_batch}\n",
    "                self.sess.run(self.train_step, feed_dict=feed_dict)\n",
    "                # Obtener estadisticas del entrenamiento\n",
    "                if iteration % validation_period == 0:\n",
    "                    iteration_history.append(iteration)\n",
    "                    # Estadisticas en el set de validacion\n",
    "                    feed_dict = {self.inputs_ph: X_val, self.labels_ph: y_val}\n",
    "                    val_loss, val_acc, val_summ = self.sess.run(\n",
    "                        [self.loss, self.accuracy, self.val_summ],\n",
    "                        feed_dict=feed_dict)\n",
    "                    writer_val.add_summary(val_summ, iteration)\n",
    "                    val_loss_history.append(val_loss)\n",
    "                    val_acc_history.append(val_acc)\n",
    "                    # Estadisticas en el set de entrenamiento\n",
    "                    feed_dict = {self.inputs_ph: X_train, self.labels_ph: y_train}\n",
    "                    train_loss, train_acc, train_summ = self.sess.run(\n",
    "                        [self.loss, self.accuracy, self.summ],\n",
    "                        feed_dict=feed_dict)\n",
    "                    writer_train.add_summary(train_summ, iteration)\n",
    "                    train_loss_history.append(train_loss)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    print('Epoch: %d/%d, iter: %d. ' %\n",
    "                          (epoch+1, self.max_epochs, iteration), end='')\n",
    "                    print('Loss (train/val): %.3f / %.3f. Val. acc: %.1f%%' %\n",
    "                          (train_loss, val_loss, val_acc * 100), end='')\n",
    "                    \n",
    "                    # Chequear condicion de early_stopping\n",
    "                    if self.early_stopping is not None:\n",
    "                        if val_loss > prev_validation_loss:\n",
    "                            validation_checks += 1\n",
    "                        else:\n",
    "                            validation_checks = 0\n",
    "                            prev_validation_loss = val_loss\n",
    "                        print(', Val. checks: %d/%d' %\n",
    "                              (validation_checks, self.early_stopping))\n",
    "                        if validation_checks >= self.early_stopping:\n",
    "                            early_stop_flag = True\n",
    "                            print('Early stopping')\n",
    "                    else:\n",
    "                        print('')\n",
    "            elap_time = time.time()-start_time\n",
    "            print(\"Epoch finished. Elapsed time %1.4f [s]\\n\" % (elap_time,))\n",
    "        writer_train.flush()\n",
    "        writer_val.flush()\n",
    "        # Guardar estadisticas en un diccionario\n",
    "        train_stats = {\n",
    "            'iteration_history': np.array(iteration_history),\n",
    "            'train_loss_history': np.array(train_loss_history),\n",
    "            'train_acc_history': np.array(train_acc_history),\n",
    "            'val_loss_history': np.array(val_loss_history),\n",
    "            'val_acc_history': np.array(val_acc_history)\n",
    "        }\n",
    "        return train_stats\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
    "        \"\"\"\n",
    "        # Obtener las probabilidades de salida de cada clase\n",
    "        predicted_proba = self.sess.run(self.proba, feed_dict={self.inputs_ph: X})\n",
    "        return predicted_proba\n",
    "    \n",
    "    def predict_label(self, X):\n",
    "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
    "        \"\"\"\n",
    "        # Obtener la probabilidad de cada clase\n",
    "        predicted_proba = self.predict_proba(X)\n",
    "        # Etiquetar segun la etiqueta mas probable\n",
    "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HUxx8YiRbDl"
   },
   "source": [
    "## Entrenamiento de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYshAaUvRaZa"
   },
   "outputs": [],
   "source": [
    "# ----- Directorio para logs\n",
    "experiment_name = \"exp_xentropy\"\n",
    "\n",
    "# --- NO TOCAR\n",
    "logdir_father = \"./tarea_1_logs/\"\n",
    "logdir = logdir_father + experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWrYVCRWpAXw"
   },
   "outputs": [],
   "source": [
    "# ----- Ejecutar si es que se quiere un directorio limpio en logdir\n",
    "# Si existe el directorio, ejecutar esta celda movera los archivos a otra\n",
    "# carpeta para limpiar el logdir elegido.\n",
    "\n",
    "%%bash -s \"$logdir\"\n",
    "if [ -d $1 ]; then\n",
    "    this_date=\"$(date \"+%H%M%S-%y%m%d\")\"\n",
    "    new_dir=\"$1_old_$this_date\"\n",
    "    echo \"moving files from $1 to $new_dir\"\n",
    "    mv $1 $new_dir\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48418
    },
    "colab_type": "code",
    "id": "xmNz6vshn1q2",
    "outputId": "4f2c58b3-5a10-4d40-a6e0-2b119f93dcec"
   },
   "outputs": [],
   "source": [
    "run_n_times = 5\n",
    "stats_history = []\n",
    "for run in range(run_n_times):\n",
    "    # ----- Creacion de MLP\n",
    "    mlp = MLPClassifier(\n",
    "        n_features=28*28,\n",
    "        layer_sizes=[25, 2],\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=15,\n",
    "        logdir=logdir+'run_%d' % run)\n",
    "\n",
    "    # ----- Entrenamiento de MLP\n",
    "    train_stats = mlp.fit(training_images, training_labels, validation_images, validation_labels)\n",
    "    stats_history.append(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uszFkplq3avX"
   },
   "source": [
    "### Tasa de acierto en validación computada sobre varias ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNS4VAkflQvE"
   },
   "outputs": [],
   "source": [
    "val_acc_history = np.array([run['val_acc_history'][-1] for run in stats_history])\n",
    "val_acc_mean = val_acc_history.mean()\n",
    "val_acc_std = val_acc_history.std()\n",
    "print('Validation accuracy %.3f +/- %.3f' % (val_acc_mean, val_acc_std))\n",
    "print(val_acc_history)\n",
    "\n",
    "train_acc_history = np.array([run['train_acc_history'][-1] for run in stats_history])\n",
    "train_acc_mean = train_acc_history.mean()\n",
    "train_acc_std = train_acc_history.std()\n",
    "print('Train accuracy %.3f +/- %.3f' % (train_acc_mean, train_acc_std))\n",
    "print(train_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDrpTQWXa19D"
   },
   "source": [
    "## Visualización de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ypv7zhabDJT"
   },
   "source": [
    "### Algunas estadísticas del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "XsDDF3BoboJS",
    "outputId": "897b64e8-6bab-4b0b-8264-bcc754ed5be1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['val_loss_history'], label='validation')\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['train_loss_history'], label='training')\n",
    "ax[0].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss evolution during training')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['val_acc_history'], label='validation')\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['train_acc_history'], label='training')\n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Accuracy evolution during training')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF87rmUxbMtW"
   },
   "source": [
    "### Estadísticas del desempeño final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "colab_type": "code",
    "id": "XKM3ashMUVZa",
    "outputId": "264a1aef-a2bc-49e2-b22c-2aac4f26c64e"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predictions):\n",
    "    \"\"\"Calcula la matriz de confusion.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        predictions: Array binario 1-D con las predicciones.\n",
    "        \n",
    "    Returns:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \"\"\"\n",
    "    # Map labels and predictions to {0, 1, 2, 3}\n",
    "    encoded_data = 2 * labels + predictions  \n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "\n",
    "def performance_metrics(TP, FP, FN, TN):\n",
    "    \"\"\"Calcula metricas de desempeño.\n",
    "    \n",
    "    Args:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Porcentaje de clasificaciones correctas del detector.\n",
    "        precision: Precision del detector.\n",
    "        recall: Recall/Sensibilidad del detector.\n",
    "    \"\"\"\n",
    "    accuracy = 100.0 * (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = 100.0 * TP / (TP + FP)\n",
    "    recall = 100.0 * TP / (TP + FN)\n",
    "    print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))\n",
    "    print('%1.4f%% Accuracy (Porcentaje de clasificaciones correctas)' % (accuracy))\n",
    "    print('%1.4f%% Precision' % (precision))\n",
    "    print('%1.4f%% Recall' % (recall))\n",
    "    print('')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "def roc_curve(labels, probabilities):\n",
    "    \"\"\"Calcula la curva ROC.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        probabilities: Array 1-D continuo en el rango [0, 1] con las\n",
    "            probabilidades de la clase 1.\n",
    "        \n",
    "    Returns:\n",
    "        tpr: Array 1-D con los valores de Tasa de Verdaderos Positivos (TPR).\n",
    "        fpr: Array 1-D con los valores de Tasa de Falsos Positivos (FPR).\n",
    "    \"\"\"\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        probabilities_with_threshold = (probabilities > threshold).astype(np.float)\n",
    "        TP, FP, FN, TN = confusion_matrix(\n",
    "            labels, \n",
    "            probabilities_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "  \n",
    "  \n",
    "def detection_performance_given_threshold(true_labels, prediction, threshold):\n",
    "    probabilities_with_threshold = (prediction > threshold).astype(np.float)\n",
    "    TP, FP, FN, TN = confusion_matrix(\n",
    "        true_labels, \n",
    "        probabilities_with_threshold)    \n",
    "    return TP, FP, FN, TN       \n",
    "\n",
    "  \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.grid('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "predicted_train_labels = mlp.predict_label(training_images)\n",
    "predicted_val_labels = mlp.predict_label(validation_images)\n",
    "predicted_test_labels = mlp.predict_label(testing_images)\n",
    "\n",
    "cnf_matrix = sk_conf_mat(testing_labels, predicted_test_labels)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['%d' % RUT_veri_number, 'no %d' % RUT_veri_number],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "\n",
    "print('Training results:')\n",
    "TP, FP, FN, TN = confusion_matrix(training_labels, predicted_train_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Validation results:')\n",
    "TP, FP, FN, TN = confusion_matrix(validation_labels, predicted_val_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Test results:')\n",
    "TP, FP, FN, TN = confusion_matrix(testing_labels, predicted_test_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "tpr, fpr = roc_curve(testing_labels, predicted_test_proba[:, 1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax[0].set_title('ROC Curve')\n",
    "ax[0].plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "ax[1].set_title('DET Curve')\n",
    "ax[1].plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('False Negative Rate')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsWfO5koe6wg"
   },
   "source": [
    "### Performance según umbral de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neruAyQbecmn"
   },
   "outputs": [],
   "source": [
    "predicted_validation_proba = mlp.predict_proba(validation_images)\n",
    "TP, FP, FN, TN = detection_performance_given_threshold(validation_labels, predicted_validation_proba[:, 1], threshold=0.5)\n",
    "print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2zFsvMobYvZ"
   },
   "source": [
    "### Visualización de clasificaciones en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 866
    },
    "colab_type": "code",
    "id": "KyzPqoEgV0ky",
    "outputId": "cb2b9ff5-9264-416a-d9e2-4fece649b943"
   },
   "outputs": [],
   "source": [
    "def show_classifications(images, labels, probabilities, result_type='TP'):\n",
    "    \"\"\" Muestra ejemplos de imagenes para tipos de errores.\n",
    "    \n",
    "    Args:\n",
    "        images: Array de dimensiones (n_ejemplos, n_pixeles) con imagenes.\n",
    "        labels: Array de dimensiones (n_ejemplos,) con las etiquetas reales.\n",
    "        probabilities: Array de dimensiones (n_ejemplos,) con las probabilidades\n",
    "            de la clase 1.\n",
    "        result_type: 'TP', 'FP', 'FN', o 'TP', tipo de error a mostrar.\n",
    "    \"\"\"\n",
    "    dict_types = {'TN': 0, 'FP': 1, 'FN': 2, 'TP': 3}\n",
    "    predictions = (probabilities > 0.5).astype(np.int32)\n",
    "    encoded_data = 2 * labels + predictions \n",
    "    useful = np.where(encoded_data == dict_types[result_type])[0]\n",
    "    size = min(4, useful.shape[0])\n",
    "    chosen = np.random.choice(useful, size=size, replace=False)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10,4))\n",
    "    for i, idx in enumerate(chosen):\n",
    "        image = images[idx, :]\n",
    "        digit = labels[idx]\n",
    "        predicted_label = predictions[idx]\n",
    "        proba = probabilities[idx]\n",
    "        ax[i].imshow(image.reshape((28, 28)))\n",
    "        ax[i].set_title(\"True Class: %d\\nPredicted Class: %d\\nProb. of Class 1: %1.4f\"\n",
    "                        % (digit, predicted_label, proba))\n",
    "        ax[i].axis('off')\n",
    "    for j in range(i+1, 4):\n",
    "        ax[j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "predicted_test_proba = predicted_test_proba[:, 1]\n",
    "\n",
    "print('True Positives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TP')\n",
    "\n",
    "print('True Negatives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TN')\n",
    "\n",
    "print('False Positive:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FP')\n",
    "\n",
    "print('False Negative:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q30tuRmImaI0"
   },
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skWOwVB_h0T-"
   },
   "outputs": [],
   "source": [
    "# ----- Descarga de ngrok para crear tunel\n",
    "%%bash\n",
    "file=\"ngrok-stable-linux-amd64.zip\"\n",
    "if [ -f \"$file\" ]\n",
    "then\n",
    "\techo \"$file already downloaded.\"\n",
    "else\n",
    "    wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "    unzip ngrok-stable-linux-amd64.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5DIdn6KhPpf"
   },
   "outputs": [],
   "source": [
    "# ----- Ejecutar despues de nueva corrida para actualizar Tensorboard\n",
    "\n",
    "LOG_DIR = logdir_father  # este es el logdir de nuestros summaries\n",
    "print(\"Showing summaries at %s\" % (LOG_DIR))\n",
    "\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "print('Click URL to open TensorBoard:')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b6uAwj7q2TA"
   },
   "source": [
    "Si quieren borrar su log_dir, utilicen lo siguiente:\n",
    "\n",
    "Ejecutar comandos en bash\n",
    "* %%bash\n",
    "\n",
    "para mirar lo que hay en el directorio actual:\n",
    "* ls \n",
    "\n",
    "para borrar la carpeta \"tarea_1_logs\"\n",
    "* rm -r tarea_1_logs\n",
    "\n",
    "buscar procesos asociados a tensorboard\n",
    "* ps aux | grep tensorboard\n",
    "\n",
    "terminar un proceso\n",
    "* kill -9 process_id (process_id es el pid del proceso)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDbgq2lP4xZy"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veKtB6KRkLIu"
   },
   "outputs": [],
   "source": [
    "#!rm -r tarea_1_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JviRuQufPeZf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea1_modificado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
